# üöÄ Ollama Gateway - P0

**OpenAI-compatible API gateway for local Ollama models with intelligent routing**

## Objectif

Permettre √† Claude-Code, Continue.dev, Cursor, et tout outil compatible OpenAI d'utiliser **vos 9 mod√®les Ollama locaux** avec routing automatique intelligent.

## Architecture

```
Claude-Code/Continue ‚Üí Gateway (localhost:4000) ‚Üí Routing Intelligent ‚Üí Mod√®les Ollama
```

## Installation (3 commandes)

```bash
cd C:\Dev\ollama-gateway

# Installer les d√©pendances
pip install -r requirements.txt

# Lancer le gateway
python main.py
```

Le gateway d√©marre sur **http://localhost:4000**

## Configuration

### Models configur√©s (config.json)

| Mod√®le | R√¥le | Tags | Priorit√© |
|--------|------|------|----------|
| deepseek-coder-v2 | coding | code, python, debug, refactor | 1 |
| deepseek-chess | chess | √©checs, chess, fen, pgn | 1 |
| gemma2 | creative | creative, story, write, poem | 2 |
| qwen2.5 | multilingual | translate, fran√ßais, english | 2 |
| mistral | general | (d√©faut) | 1 |
| llama3.2 | fast | quick, fast, short | 3 |
| + 3 mod√®les chess sp√©cialis√©s | | | |

### Routing Intelligent

Le gateway analyse votre prompt et route automatiquement :

- **"Write a Python function"** ‚Üí deepseek-coder-v2
- **"Quelle est la meilleure ouverture aux √©checs ?"** ‚Üí deepseek-chess
- **"Translate to French"** ‚Üí qwen2.5
- **"Write a story"** ‚Üí gemma2
- **"Quick answer"** ‚Üí llama3.2
- **Autre** ‚Üí mistral

## Utilisation

### 1. Avec Claude-Code / Continue.dev

Dans votre configuration VSCode (Continue ou Claude-Code) :

```json
{
  "models": [
    {
      "title": "Ollama Local (Auto-Routing)",
      "provider": "openai",
      "model": "auto",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    }
  ]
}
```

### 2. Avec Cursor

Settings ‚Üí Models ‚Üí Add Custom Model :
- Provider: OpenAI Compatible
- Base URL: `http://localhost:4000/v1`
- Model: `auto`

### 3. Avec curl (test)

```bash
# Test simple
curl http://localhost:4000/health

# Chat completion (auto-routing)
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [
      {"role": "user", "content": "Write a Python function to reverse a string"}
    ]
  }'
```

### 4. Test de routing

```bash
curl -X POST http://localhost:4000/gateway/route \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain the Sicilian Defense in chess"}'
```

R√©ponse :
```json
{
  "prompt": "Explain the Sicilian Defense in chess",
  "selected_model": "deepseek-chess:latest",
  "reason": "Best for chess (matched: √©checs, chess)"
}
```

## Endpoints

### OpenAI-Compatible

- `POST /v1/chat/completions` - Chat completion (avec routing automatique)
- `GET /v1/models` - Liste des mod√®les disponibles

### Gateway-Specific

- `GET /health` - √âtat de sant√©
- `GET /gateway/models` - Configuration des mod√®les
- `POST /gateway/route` - Tester le routing

## Avantages

‚úÖ **Compatible OpenAI** - Fonctionne avec tous les outils existants
‚úÖ **Routing automatique** - Choisit le meilleur mod√®le selon le prompt
‚úÖ **100% local** - Aucune donn√©e envoy√©e au cloud
‚úÖ **Streaming support√©** - R√©ponses en temps r√©el
‚úÖ **√âconomie tokens Claude** - Claude d√©l√®gue aux mod√®les locaux
‚úÖ **Phase 2 ready** - Peut fonctionner 100% sans Claude

## Workflow Typique

**Avant** (sans Gateway) :
```
Vous ‚Üí Question ‚Üí Claude ‚Üí R√©ponse Claude (co√ªteux en tokens)
```

**Maintenant** (avec Gateway) :
```
Vous ‚Üí Question code ‚Üí Claude-Code ‚Üí Gateway ‚Üí deepseek-coder ‚Üí R√©ponse
Vous ‚Üí Question chess ‚Üí Claude-Code ‚Üí Gateway ‚Üí deepseek-chess ‚Üí R√©ponse
```

**√âconomie** : ~10-20x moins de tokens Claude consomm√©s

## Logs

Le gateway log toutes les d√©cisions de routing :

```
2025-01-18 10:30:15 - INFO - Received chat completion request
2025-01-18 10:30:15 - INFO - Routing: deepseek-coder-v2:latest - Best for coding (matched: code, python)
```

## Prochaines √âtapes (P1)

- Dashboard autonome avec conversation
- Visualisation temps r√©el
- Statistiques d'utilisation
- Historique des conversations
- Export/Import de configurations

## Troubleshooting

**Gateway ne d√©marre pas** :
- V√©rifier que Python 3.9+ est install√©
- V√©rifier que Ollama tourne (`ollama serve`)

**Mod√®le non trouv√©** :
- Lister vos mod√®les : `ollama list`
- V√©rifier config.json correspond √† vos mod√®les install√©s

**Pas de r√©ponse** :
- V√©rifier logs du gateway
- Tester Ollama directement : `ollama run mistral "Hello"`

## Support

- Gateway cr√©√© pour usage avec vos 9 mod√®les Ollama locaux
- Configuration adapt√©e √† vos mod√®les sp√©cifiques
- Pr√™t pour int√©gration Claude-Code / Continue / Cursor
