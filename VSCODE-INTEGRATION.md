# üîå Int√©gration VSCode - Ollama Gateway

Utilisez vos 9 mod√®les Ollama locaux directement dans VSCode avec l'orchestration multi-mod√®le !

## Extensions Support√©es

### 1. Continue.dev (Recommand√©) ‚≠ê

**Installation** :
1. Installez l'extension : [Continue - VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
2. Ouvrez les param√®tres : `Ctrl+Shift+P` ‚Üí "Continue: Open config.json"

**Configuration** :

```json
{
  "models": [
    {
      "title": "ü§ñ Orchestrate (Multi-AI)",
      "provider": "openai",
      "model": "orchestrate",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "üéØ Auto Routing",
      "provider": "openai",
      "model": "auto",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "üíª DeepSeek Coder",
      "provider": "openai",
      "model": "deepseek-coder-v2:latest",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "‚ôüÔ∏è Chess Expert",
      "provider": "openai",
      "model": "deepseek-chess:latest",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "üåç Multilingual (Qwen)",
      "provider": "openai",
      "model": "qwen2.5:latest",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "‚úçÔ∏è Creative Writer",
      "provider": "openai",
      "model": "gemma2:latest",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    },
    {
      "title": "‚ö° Fast (Llama3.2)",
      "provider": "openai",
      "model": "llama3.2:latest",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed"
    }
  ],
  "tabAutocompleteModel": {
    "title": "Autocomplete",
    "provider": "openai",
    "model": "deepseek-coder-v2:latest",
    "apiBase": "http://localhost:4000/v1",
    "apiKey": "not-needed"
  }
}
```

**Utilisation** :
- `Ctrl+L` : Chat avec le mod√®le s√©lectionn√©
- `Ctrl+I` : Edit inline avec suggestions
- S√©lectionnez "ü§ñ Orchestrate" pour les t√¢ches complexes
- S√©lectionnez "üíª DeepSeek Coder" pour le code pur

---

### 2. Cline (Claude Code) üîß

**Installation** :
1. Installez : [Cline - VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
2. Param√®tres : `Ctrl+Shift+P` ‚Üí "Cline: Open Settings"

**Configuration** :

```json
{
  "apiProvider": "openai-compatible",
  "openAiCompatibleEndpoint": "http://localhost:4000/v1",
  "openAiCompatibleApiKey": "not-needed",
  "openAiCompatibleModelId": "orchestrate"
}
```

**Pour changer de mod√®le** :
- `orchestrate` - Orchestration multi-mod√®le
- `auto` - Routing intelligent
- `deepseek-coder-v2:latest` - Code sp√©cialis√©

---

### 3. Cody by Sourcegraph ü¶â

**Installation** :
1. Installez : [Cody AI - VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=sourcegraph.cody-ai)
2. Settings ‚Üí Extensions ‚Üí Cody

**Configuration** :

```json
{
  "cody.serverEndpoint": "http://localhost:4000/v1",
  "cody.customHeaders": {
    "Authorization": "Bearer not-needed"
  },
  "cody.experimental.chatModel": "orchestrate"
}
```

---

### 4. Tabnine (Autocomplete) üìù

**Installation** :
1. Installez : [Tabnine - VSCode Marketplace](https://marketplace.visualstudio.com/items?itemName=TabNine.tabnine-vscode)
2. Tabnine Settings ‚Üí Custom Endpoint

**Configuration** :
- Custom endpoint: `http://localhost:4000/v1`
- Model: `deepseek-coder-v2:latest`
- API Key: `not-needed`

---

## Workflow Recommand√©s

### Pour le d√©veloppement g√©n√©ral
```
Continue.dev avec "üéØ Auto Routing"
‚Üì
Le gateway route automatiquement vers le bon mod√®le
```

### Pour les t√¢ches complexes
```
Continue.dev avec "ü§ñ Orchestrate (Multi-AI)"
‚Üì
Exemple: "Refactor this code and add tests"
‚Üì
deepseek-coder analyse + gemma2 documente + qwen2.5 traduit
```

### Pour le code pur
```
Continue.dev avec "üíª DeepSeek Coder"
‚Üì
Coding sp√©cialis√© sans routing
```

---

## Exemples de Prompts

### Avec Orchestration (model: orchestrate)

**Prompt** :
```
Compare Python async/await vs JavaScript promises,
then refactor this code to use async patterns in both languages.
```

**R√©sultat** :
- deepseek-coder explique Python async
- deepseek-coder explique JS promises
- deepseek-coder refactorise les deux versions
- qwen2.5 synth√©tise la comparaison

### Avec Auto Routing (model: auto)

**Prompt** :
```
Fix this bug in my chess engine
```

**R√©sultat** :
- Gateway d√©tecte "chess" ‚Üí route vers deepseek-chess
- R√©ponse sp√©cialis√©e en strat√©gie d'√©checs

---

## Configuration Avanc√©e

### Utiliser diff√©rents mod√®les par contexte

**Continue.dev - config.json** :
```json
{
  "contextProviders": [
    {
      "name": "code",
      "params": {
        "model": "deepseek-coder-v2:latest"
      }
    },
    {
      "name": "docs",
      "params": {
        "model": "qwen2.5:latest"
      }
    }
  ]
}
```

### Ajouter des instructions syst√®me personnalis√©es

```json
{
  "models": [
    {
      "title": "Code Reviewer",
      "provider": "openai",
      "model": "orchestrate",
      "apiBase": "http://localhost:4000/v1",
      "apiKey": "not-needed",
      "systemMessage": "You are a senior code reviewer. Focus on: 1) Security 2) Performance 3) Best practices"
    }
  ]
}
```

---

## D√©marrage Rapide

### 1. D√©marrer le gateway
```bash
cd C:\Dev\ollama-gateway
python main.py
```

### 2. V√©rifier la connexion
```bash
curl http://localhost:4000/health
```

### 3. Installer Continue.dev
- Ouvrir VSCode
- Extensions ‚Üí Rechercher "Continue"
- Installer
- Copier la config ci-dessus dans config.json

### 4. Tester
- `Ctrl+L` dans VSCode
- S√©lectionner "ü§ñ Orchestrate (Multi-AI)"
- Poser une question complexe

---

## Troubleshooting

### "Connection refused"
```bash
# V√©rifier que le gateway tourne
curl http://localhost:4000/health

# Red√©marrer si n√©cessaire
python main.py
```

### "Model not found"
- V√©rifier que Ollama tourne : `ollama list`
- V√©rifier config.json correspond √† vos mod√®les install√©s

### "Slow response"
- Les mod√®les lourds (deepseek-coder-v2) prennent 5-10s
- Utilisez "‚ö° Fast (Llama3.2)" pour des r√©ponses rapides
- L'orchestration prend 20-60s (multiple mod√®les)

---

## Comparaison Extensions

| Extension | Autocomplete | Chat | Edit | Orchestration |
|-----------|-------------|------|------|---------------|
| Continue.dev | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Cline | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ |
| Cody | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Tabnine | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |

**Recommandation** : Continue.dev pour fonctionnalit√©s compl√®tes

---

## Roadmap

- [ ] Support streaming dans Continue.dev
- [ ] Presets de configuration pour diff√©rents langages
- [ ] Int√©gration avec GitHub Copilot Chat
- [ ] Dashboard VSCode int√©gr√©
- [ ] Statistiques d'utilisation par extension

---

## Support

**Gateway ne r√©pond pas** :
1. V√©rifier `python main.py` est actif
2. Tester `curl http://localhost:4000/health`
3. V√©rifier les logs du gateway

**Extension ne se connecte pas** :
1. V√©rifier l'URL est `http://localhost:4000/v1` (avec /v1)
2. V√©rifier apiKey est `"not-needed"` (requis m√™me si vide)
3. Red√©marrer VSCode apr√®s changement de config

---

**Cr√©√© pour une int√©gration transparente de vos 9 mod√®les Ollama locaux dans VSCode** üöÄ
